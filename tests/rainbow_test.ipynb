{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "\n",
    "import gym\n",
    "import sys\n",
    "import torch\n",
    "import tester\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, Javascript\n",
    "\n",
    "import pytorch_drl.models.rainbow_models as models\n",
    "from notebook_utils import plot, mean_filter\n",
    "from pytorch_drl.algs.rainbow import Rainbow\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name)\n",
    "env.seed(0)\n",
    "\n",
    "state_size =  env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"State size:\", state_size, \"\\nAction size:\", action_size)\n",
    "\n",
    "tmax = 1000\n",
    "n_episodes = 2000\n",
    "\n",
    "vmin = 0.0\n",
    "vmax = 200.0\n",
    "\n",
    "seed = 0\n",
    "atoms = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define networks for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainbow_model = models.RainbowNetwork(state_size, action_size, atoms)\n",
    "\n",
    "dqn_model = models.DQNNetwork(state_size, action_size)\n",
    "\n",
    "ns_model = models.NoisyRainbowNetwork(state_size, action_size)\n",
    "\n",
    "dl_model = models.DuelingRainbowNetwork(state_size, action_size)\n",
    "\n",
    "nd_model = models.NoisyDuelingRainbowNetwork(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow Test\n",
    "\n",
    "Test the standard rainbow algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vmin = -250\n",
    "vmax = 250\n",
    "\n",
    "seed = 0\n",
    "atoms = 151\n",
    "\n",
    "#rainbow_model = models.RainbowNetwork(state_size, action_size, atoms)\n",
    "rainbow_model = models.NoisyDuelingRainbowNetwork(state_size, action_size)\n",
    "\n",
    "# init agent:\n",
    "agent = Rainbow(action_size,\n",
    "                 model=rainbow_model,\n",
    "                 gamma=0.99,\n",
    "                 #lr=5e-4,\n",
    "                 learn_every=4,\n",
    "                 buf_size=int(1e5),\n",
    "                 batch_size=64,\n",
    "                 tau=1e-3,\n",
    "                 device=device,\n",
    "                 seed=0,\n",
    "                 ddqn=True, #double q learning\n",
    "                 categorical_dqn=False, #use c51 algortihm\n",
    "                 vmin=vmin, #categorical_dqn: vmin\n",
    "                 vmax=vmax, #categorical_dqn: vmax\n",
    "                 atoms=atoms,#categorical_dqn: atoms\n",
    "                 prioritized_replay=True, # use per\n",
    "                 is_beta=0.6, # per: importance sampling\n",
    "                 beta_horz=8e4, #per: beta\n",
    "                 pr_alpha=0.2, # per: alpha\n",
    "                 nstep=True, #use nstep returns\n",
    "                 n=3, #n-step: n\n",
    "                 noisy=False, #use nosiy linear layers\n",
    "                )\n",
    "\n",
    "\n",
    "alg_name = \"rainbow_{}\".format(env_name)\n",
    "max_score = 200.\n",
    "scores = agent.train(env, tmax, n_episodes, alg_name, max_score)\n",
    "# plot the training:\n",
    "plot(scores, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Agent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(env, max_t, render=True, num_of_episodes=5, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Test\n",
    "\n",
    "Test the DQN algorithm with Double Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init agent:\n",
    "dqn_model = models.DQNNetwork(state_size, action_size, H=64)\n",
    "\n",
    "dqn_agent = Rainbow(action_size,\n",
    "                     model=dqn_model,\n",
    "                     ddqn=True,\n",
    "                     gamma = 0.99,\n",
    "                     lr = 5e-4,\n",
    "                     learn_every = 4,\n",
    "                     buf_size = int(1e5),\n",
    "                     batch_size = 64,\n",
    "                     tau = 1e-3,\n",
    "                     device = device)\n",
    "\n",
    "alg_name = \"dqn_{}\".format(env_name)\n",
    "max_score = 200\n",
    "scores = dqn_agent.train(env, tmax, n_episodes, alg_name, max_score)\n",
    "plot(scores, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Agent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.test(env, tmax, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR - DQN Test\n",
    "\n",
    "Test the DQN algorithm with quantized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_quants = 51\n",
    "qrdqn_model = models.QRDQNNetwork(state_size, action_size, n_quants)\n",
    "\n",
    "qr_dqn_agent = Rainbow(action_size,\n",
    "                         model=qrdqn_model,\n",
    "                         ddqn=True,\n",
    "                         gamma = 0.99,\n",
    "                         lr = 5e-4,\n",
    "                         n_quants=n_quants,\n",
    "                         quantile_regression=True,\n",
    "                         learn_every = 4,\n",
    "                         buf_size = int(1e5),\n",
    "                         batch_size = 64,\n",
    "                         tau = 1e-3,\n",
    "                         device = device)\n",
    "\n",
    "alg_name = \"qr_dqn_{}\".format(env_name)\n",
    "max_score = 200\n",
    "scores = qr_dqn_agent.train(env, tmax, n_episodes, alg_name, max_score)\n",
    "plot(scores, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Agent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_dqn_agent.test(env, tmax, render=True, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow with Quantile Regression Test\n",
    "\n",
    "Test the Rainbow algorithm with quantized regression instead for distributional approach (C51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_quants = 51\n",
    "\n",
    "qr_rainbow_model = models.QRRainbowNetwork(state_size, action_size, n_quants)\n",
    "\n",
    "\n",
    "\n",
    "# init agent:\n",
    "qr_rainbow_agent = Rainbow(action_size,\n",
    "                             model=qr_rainbow_model,\n",
    "                             gamma=0.99,\n",
    "                             #lr=5e-4,\n",
    "                             learn_every=4,\n",
    "                             n_quants=n_quants,\n",
    "                             quantile_regression=True,\n",
    "                             buf_size=int(1e5),\n",
    "                             batch_size=64,\n",
    "                             tau=5e-3,\n",
    "                             device=device,\n",
    "                             seed=0,\n",
    "                             ddqn=True, #double q learning\n",
    "                             prioritized_replay=True, # use per\n",
    "                             is_beta=0.4, # per: importance sampling\n",
    "                             beta_horz=1e6, #per: beta\n",
    "                             pr_alpha=0.6, # per: alpha\n",
    "                             nstep=True, #use nstep returns\n",
    "                             n=3, #n-step: n\n",
    "                             noisy=False, #use nosiy linear layers\n",
    "                            )\n",
    "\n",
    "\n",
    "alg_name = \"qr_rainbow_{}\".format(env_name)\n",
    "max_score = 200\n",
    "scores = qr_rainbow_agent.train(env, tmax, n_episodes, alg_name, max_score)\n",
    "plot(scores, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Agent Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_rainbow_agent.test(env, tmax, render=True, n_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-agents)",
   "language": "python",
   "name": "ml-agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
